{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5648bfa7-470a-451e-8f7c-98d9191ad526",
   "metadata": {},
   "source": [
    "# Perceptron learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e138ec5f-ba0d-46b8-b985-72a12bc55158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d615818a-c57f-4ddc-abe9-6c43a436c97f",
   "metadata": {},
   "source": [
    "Perceptron learning is an iterative algorithm that converges to the appropriate weights for a single perceptron, given a learnable training set. If the training set is not learnable, the algorithm will not converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67fb04-a352-4982-89a8-c34705739dd5",
   "metadata": {},
   "source": [
    "**Perceptron Learning Algorithm**\n",
    "\n",
    "Given a dataset $X$ of $m$ observations $x \\in \\mathcal{R}^{1 \\times n}$, an outcome vector $y \\in \\mathcal{R}^{m \\times 1}$, we wish to find a weight vector $w \\in \\mathcal{R}^{n \\times 1}$ such that $y_i = A(\\mathbf{x}_i \\cdot \\mathbf{w})$ for each $0 \\leq i < m$. In matrix form: $A(\\mathbf{X} \\mathbf{w}) = \\mathbf{y}$. The last element of each observation vector $\\mathbf{x}$ will be -1 to account for the bias term.\n",
    "\n",
    "1. Initialize $ \\mathbf{w} $ as a random vector.\n",
    "2. **For each epoch**:\n",
    "    - For each $ (\\mathbf{x}_i, y_i)$ in the training set:\n",
    "      - Compute $ \\hat{y} = A(\\mathbf{x}_i \\cdot \\mathbf{w}) $.\n",
    "      - Update $ \\mathbf{w} $ as:\n",
    "        $$\n",
    "        \\mathbf{w} \\leftarrow \\mathbf{w} + (y_i - \\hat{y}) \\lambda \\mathbf{x}_i\n",
    "        $$\n",
    "3. Compute accuracy:\n",
    "   $$\n",
    "   \\text{accuracy} = 1 - \\frac{\\sum_i |f(\\mathbf{x}_i) - A(\\mathbf{x}_i \\cdot \\mathbf{w})|}{\\text{len}(\\text{training\\_set})}\n",
    "   $$\n",
    "4. Return $ (\\mathbf{w}, \\text{accuracy}) $.\n",
    "\n",
    "The learning rate $\\lambda$ determines the rate of convergence. In practice $\\lambda$ can be close to, but not greater than, 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b3e9ea-87f9-4b22-97df-b54918996b18",
   "metadata": {},
   "source": [
    "We'll define the data set as a matrix $X$ and a vector $y$ using numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1450bbf4-1e27-4d96-9de7-18e62c6c7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The dataset corresponding to boolean AND. \n",
    "X = np.array([\n",
    "    [0, 0, -1],\n",
    "    [0, 1, -1],\n",
    "    [1, 0, -1],\n",
    "    [1, 1, -1]\n",
    "])\n",
    "y = np.array([0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c13c656-6e0e-400f-8e72-bf4fa1207425",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The heaviside step function\n",
    "def A(x):\n",
    "    return np.heaviside(x,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a1358b-87a6-4011-9bf8-356cf2cb05ed",
   "metadata": {},
   "source": [
    "In numpy and tensorflow you will find extracting data elements with the appropriate dimensionality can be difficult. To get to i_th row of $X$, use the syntax\n",
    "\n",
    "`x = X[i:i+1]`\n",
    "\n",
    "this will ensure `x` is a matrix and not a vector (shape will be 2D not 1D)\n",
    "\n",
    "You can now multiply `x` and `w` with `x @ w`\n",
    "\n",
    "If you want `x` to be a 1D vector, you can use `x = X[i]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ab507d-25dd-42ba-a5b5-e83f37d8e4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Weights: [1.26181597 0.59569597 1.46414428]\n",
      "Final Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "def perceptron_learn(X,y,lr, epochs):\n",
    "    w = np.random.rand(len(X[0]));\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(X)):\n",
    "            x = X[i]\n",
    "            yHat = A((x @ w).flatten())\n",
    "            add = (y[i]-yHat) * x * lr\n",
    "            w = w + add\n",
    "            \n",
    "    results = [abs(y[i] - A((X[i:i+1] @ w)).flatten()) for i in range(len(y))]\n",
    "    accuracy = float(1 - np.sum(results)/float(len(y)))\n",
    "    return w, accuracy\n",
    "\n",
    "\n",
    "weights, final_accuracy = perceptron_learn(X,y,0.5,1000)\n",
    "print(\"Final Weights:\", weights)\n",
    "print(\"Final Accuracy:\", final_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "526416fd-e451-49b3-be54-ae8fac4054b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def i_thBoolFunction(num):\n",
    "    inputs = list(itertools.product([0, 1], repeat=num))\n",
    "    return np.array(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec1f73a2-5a02-4b4e-879a-9402ace531d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_function = i_thBoolFunction(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30880a1a-3cc2-45bf-9e42-be3d74dca5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 0: Weights: [0.1717245  0.40363241 0.96138224], Accuracy: 1.0\n",
      "Function 1: Weights: [1.47729183 0.5932078  1.58662047], Accuracy: 1.0\n",
      "Function 2: Weights: [ 1.39044923 -1.23253537  0.43362006], Accuracy: 1.0\n",
      "Function 3: Weights: [1.39029472 0.12184723 0.492055  ], Accuracy: 1.0\n",
      "Function 4: Weights: [-0.25473802  0.94253712  0.79187205], Accuracy: 1.0\n",
      "Function 5: Weights: [0.06846311 0.64488627 0.14026048], Accuracy: 1.0\n",
      "Function 6: Weights: [-0.69947461 -0.18326653 -0.45866788], Accuracy: 0.5, UNLEARNABLE\n",
      "Function 7: Weights: [0.51832114 0.98563098 0.09036566], Accuracy: 1.0\n",
      "Function 8: Weights: [-0.89645833 -0.88305321 -0.41771804], Accuracy: 1.0\n",
      "Function 9: Weights: [0.61420813 0.08284511 0.31840912], Accuracy: 0.5, UNLEARNABLE\n",
      "Function 10: Weights: [ 0.25011421 -1.05719349 -0.3188436 ], Accuracy: 1.0\n",
      "Function 11: Weights: [ 1.21283494 -0.57048459 -0.14849662], Accuracy: 1.0\n",
      "Function 12: Weights: [-1.07047573 -0.2326307  -0.34984   ], Accuracy: 1.0\n",
      "Function 13: Weights: [-0.34250735  1.09756233 -0.0405274 ], Accuracy: 1.0\n",
      "Function 14: Weights: [-1.02652406 -0.76738104 -1.31510487], Accuracy: 1.0\n",
      "Function 15: Weights: [ 0.69216539  0.42904967 -0.23092526], Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    weights, accuracy = perceptron_learn(X, boolean_function[i], 0.5, 1000)\n",
    "    if accuracy != 1:\n",
    "        print(f\"Function {i}: Weights: {weights}, Accuracy: {accuracy}, UNLEARNABLE\")\n",
    "    else:\n",
    "        print(f\"Function {i}: Weights: {weights}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d5a54-0911-4650-8796-1366547b4099",
   "metadata": {},
   "source": [
    "**Assignment**: Determine the appropriate weight vector $w$ for the boolean function AND. Then check all 16 boolean functions on 2 variables and print weights for each *learnable* function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82a2d5-0301-478b-8626-2abb9e435ab6",
   "metadata": {},
   "source": [
    "*Sub-assignment*: You may want to write a function to generate the dataset for the i_th boolean function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
